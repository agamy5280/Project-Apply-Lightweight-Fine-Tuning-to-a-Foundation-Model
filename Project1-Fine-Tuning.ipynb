{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce651589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/student/.local/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/student/.local/lib/python3.10/site-packages (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/student/.local/lib/python3.10/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/student/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface_hub) (4.10.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c770cfff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/student/.local/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/student/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: packaging in /home/student/.local/lib/python3.10/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bb7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import get_peft_model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f48c635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label'],\n",
       "    num_rows: 4459\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=23\n",
    ")\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# View the dataset characteristics\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6221f869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 4459\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"sms\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01dcf541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"not spam\", 1: \"spam\"},\n",
    "    label2id={\"not spam\": 0, \"spam\": 1},\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419a6cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/1400 06:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.054353</td>\n",
       "      <td>0.989238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.045239</td>\n",
       "      <td>0.990135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.058476</td>\n",
       "      <td>0.990135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.060904</td>\n",
       "      <td>0.990135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.060663</td>\n",
       "      <td>0.991031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.064270</td>\n",
       "      <td>0.991031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.066205</td>\n",
       "      <td>0.991031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.067723</td>\n",
       "      <td>0.991031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.068402</td>\n",
       "      <td>0.991031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.068639</td>\n",
       "      <td>0.991031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-140 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-280 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-420 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-560 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-700 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-840 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-980 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-1120 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-1260 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-1400 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1400, training_loss=0.013904576519770282, metrics={'train_runtime': 362.1052, 'train_samples_per_second': 123.141, 'train_steps_per_second': 3.866, 'total_flos': 839113443159492.0, 'train_loss': 0.013904576519770282, 'epoch': 10.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/spam_not_spam\",\n",
    "        # Set the learning rate\n",
    "        learning_rate = 2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size = 32,\n",
    "        per_device_eval_batch_size = 32,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1cad4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normal_model_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b30ed",
   "metadata": {},
   "source": [
    "## LORA finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d33f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"transformer.layer.*.attention.q_lin\", \"transformer.layer.*.attention.k_lin\", \"transformer.layer.*.attention.v_lin\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deda8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3a87b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 68,134,658 || trainable%: 1.7313479433623928\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97453f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"distilbert-base-uncased-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ce5a2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abedeb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/1400 02:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.260489</td>\n",
       "      <td>0.870852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152965</td>\n",
       "      <td>0.956054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.106340</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.086092</td>\n",
       "      <td>0.973094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.075747</td>\n",
       "      <td>0.974888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.974888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.066818</td>\n",
       "      <td>0.975785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.064816</td>\n",
       "      <td>0.975785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.063384</td>\n",
       "      <td>0.975785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.063043</td>\n",
       "      <td>0.975785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-140 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-280 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-420 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-560 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-700 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-840 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-980 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-1120 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-1260 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/spam_not_spam/checkpoint-1400 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1400, training_loss=0.12888518469674246, metrics={'train_runtime': 133.3414, 'train_samples_per_second': 334.405, 'train_steps_per_second': 10.499, 'total_flos': 859244312853384.0, 'train_loss': 0.12888518469674246, 'epoch': 10.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/spam_not_spam\",\n",
    "        # Set the learning rate\n",
    "        learning_rate = 2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size = 32,\n",
    "        per_device_eval_batch_size = 32,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fa30f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_model_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f4c32fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between LoRA Model and Normal Model:\n",
      "Accuracy Comparison:\n",
      "LoRA Model Accuracy: 0.9757847533632287\n",
      "Normal Model Accuracy: 0.9901345291479821\n",
      "Accuracy Difference: -0.014349775784753382\n",
      "\n",
      "Loss Comparison:\n",
      "LoRA Model Loss: 0.06304305791854858\n",
      "Normal Model Loss: 0.04523885250091553\n",
      "Loss Difference: 0.017804205417633057\n",
      "\n",
      "Runtime Comparison:\n",
      "LoRA Model Runtime: 2.8763\n",
      "Normal Model Runtime: 2.7443\n",
      "Runtime Difference: 0.13200000000000012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_models(lora_model_results, normal_model_results):\n",
    "    print(\"Comparison between LoRA Model and Normal Model:\")\n",
    "    \n",
    "    lora_accuracy = lora_model_results.get(\"eval_accuracy\", None)\n",
    "    normal_accuracy = normal_model_results.get(\"eval_accuracy\", None)\n",
    "    \n",
    "    print(f\"Accuracy Comparison:\")\n",
    "    print(f\"LoRA Model Accuracy: {lora_accuracy}\")\n",
    "    print(f\"Normal Model Accuracy: {normal_accuracy}\")\n",
    "    print(f\"Accuracy Difference: {lora_accuracy - normal_accuracy}\\n\")\n",
    "\n",
    "    lora_loss = lora_model_results.get(\"eval_loss\", None)\n",
    "    normal_loss = normal_model_results.get(\"eval_loss\", None)\n",
    "    \n",
    "    print(f\"Loss Comparison:\")\n",
    "    print(f\"LoRA Model Loss: {lora_loss}\")\n",
    "    print(f\"Normal Model Loss: {normal_loss}\")\n",
    "    print(f\"Loss Difference: {lora_loss - normal_loss}\\n\")\n",
    "\n",
    "    lora_runtime = lora_model_results.get(\"eval_runtime\", None)\n",
    "    normal_runtime = normal_model_results.get(\"eval_runtime\", None)\n",
    "    \n",
    "    print(f\"Runtime Comparison:\")\n",
    "    print(f\"LoRA Model Runtime: {lora_runtime}\")\n",
    "    print(f\"Normal Model Runtime: {normal_runtime}\")\n",
    "    print(f\"Runtime Difference: {lora_runtime - normal_runtime}\\n\")\n",
    "    \n",
    "compare_models(lora_model_results, normal_model_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
